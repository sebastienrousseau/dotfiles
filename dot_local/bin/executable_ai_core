#!/usr/bin/env bash
## AI Core â€” Central wrapper for AI operations in the Predictive Shell.
##
## Manages local LLM (llamafile) and fallback to GitHub Copilot CLI.
## Handles model downloading, server management, and query routing.
##
## # Requirements
## - curl: For downloading models and API requests
## - nc (netcat): For checking server availability
## - jq: For parsing JSON responses
## - llamafile (optional): Local LLM server
## - gh copilot (optional): GitHub Copilot CLI fallback
##
## # Usage
## ai_core status              # Check AI subsystem status
## ai_core download            # Download local LLM model (~4GB)
## ai_core start               # Start local LLM server
## ai_core query "prompt"      # Query the AI (auto-selects provider)
##
## # Platform Notes
## - Linux/macOS: Full support
## - WSL: Supported with llamafile or Copilot
## - Model stored in $XDG_DATA_HOME/ai/models/

set -euo pipefail

# Configuration
LLAMAFILE_URL="https://huggingface.co/jart/inexpensive-llm/resolve/main/llamafile-server-0.1-llava-v1.5-7b-q4"
MODEL_DIR="${XDG_DATA_HOME:-$HOME/.local/share}/ai/models"
MODEL_PATH="$MODEL_DIR/llava-v1.5-7b-q4.llamafile"
# Default to port 8080 for llamafile server
SERVER_PORT=8080

# Ensure model directory exists
ensure_model_dir() {
    if [ ! -d "$MODEL_DIR" ]; then
        mkdir -p "$MODEL_DIR"
    fi
}

# Check if we have a local LLM available
check_local_llm() {
    if [ -f "$MODEL_PATH" ]; then
        return 0
    else
        return 1
    fi
}

# Download the model (interactive only)
download_model() {
    ensure_model_dir
    echo "Local LLM model not found at $MODEL_PATH"
    echo "This feature requires downloading a localized LLM (~4GB)."
    read -p "Do you want to download it now? (y/n) " -n 1 -r
    echo
    if [[ $REPLY =~ ^[Yy]$ ]]; then
        echo "Downloading llamafile..."
        # Note: curl -L is used to follow redirects
        if curl -L -o "$MODEL_PATH" "$LLAMAFILE_URL"; then
             chmod +x "$MODEL_PATH"
             echo "Download complete. Model saved to $MODEL_PATH"
        else
            echo "Download failed."
            return 1
        fi
    else
        echo "Skipping download. AI features may be limited."
        return 1
    fi
}

# Verify status of AI subsystem
status() {
    echo "--- Predictive Shell AI Core Status ---"
    if check_local_llm; then
        echo "Local LLM: Installed ($MODEL_PATH)"
    else
        echo "Local LLM: Not installed"
    fi

    # Check for other providers
    if command -v gh &> /dev/null && gh copilot --version &> /dev/null; then
         echo "GitHub Copilot CLI: Available"
    else
         echo "GitHub Copilot CLI: Not available"
    fi
}

# Start the server (background)
start_server() {
    if ! check_local_llm; then
        download_model || return 1
    fi

    echo "Starting Local LLM Server..."
    "$MODEL_PATH" --server --nobrowser --port "$SERVER_PORT" &
    echo "Server started on port $SERVER_PORT"
}

# Query the LLM
# Usage: ai_core query "Your prompt here"
query() {
    local prompt="$1"

    # Priority 1: Local LLM Server if running
    if nc -z localhost "$SERVER_PORT" 2>/dev/null; then
         # Simple curl based query to llamafile standard endpoint
         # Note: This is a simplified example payload
         curl -s -X POST "http://localhost:$SERVER_PORT/completion" \
            -H "Content-Type: application/json" \
            -d "{\"prompt\": \"$prompt\", \"n_predict\": 128}" | jq -r .content
         return
    fi

    # Priority 2: GitHub Copilot CLI
    if command -v gh &> /dev/null && gh copilot --version &> /dev/null; then
        # Use gh copilot explain/suggest
        # This wrapper might need adjustment based on how we want to use it
        # generally 'gh copilot explain' is interactive.
        echo "Local server not running. Using GitHub Copilot..."
        gh copilot explain "$prompt"
        return
    fi

    echo "No AI provider available. Run 'ai_core start' or install GitHub Copilot CLI."
}


case "$1" in
    status)
        status
        ;;
    download)
        download_model
        ;;
    start)
        start_server
        ;;
    query)
        shift
        query "$@"
        ;;
    *)
        echo "Usage: $0 {status|download|start|query}"
        exit 1
        ;;
esac
